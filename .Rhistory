#Try ridge regression
train.mat = model.matrix(Apps~., data=College.train)
test.mat = model.matrix(Apps~., data=College.test)
grid = 10 ^ seq(4, -2, length=100)
mod.ridge = cv.glmnet(train.mat, College.train[, "Apps"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = mod.ridge$lambda.min
lambda.best
mod.ridge
ridge.pred = predict(mod.ridge, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - ridge.pred)^2)
#LASSO
#Pick lamda using training set and report error from test set
mod.lasso = cv.glmnet(train.mat, College.train[, "Apps"], alpha=1, lambda=grid, thresh=1e-12)
lambda.best = mod.lasso$lambda.min
lambda.best
lasso.pred = predict(mod.lasso, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - lasso.pred)^2)
#Attain the coefficientrs, USE ALL DATA HERE! when finding the final model
mod.lasso = glmnet(model.matrix(Apps~., data=College), College[, "Apps"], alpha=1)
predict(mod.lasso, s=lambda.best, type="coefficients")
library(pls)
#Try PCR
pcr.fit = pcr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
pcr.pred = predict(pcr.fit, College.test, ncomp=10)
mean((College.test[, "Apps"] - data.frame(pcr.pred))^2)
pls.fit = plsr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pls.fit, val.type="MSEP")
pls.pred = predict(pls.fit, College.test, ncomp=10)
mean((College.test[, "Apps"] - data.frame(pls.pred))^2
mean((College.test[, "Apps"] - data.frame(pls.pred))^2)
mean((College.test[, "Apps"] - data.frame(pls.pred))^2)
coefficients(pcr.fit)
lm_predictions <- predict(lm_fit , newdata = College.test , type="Coefficients")
lm_predictions <- predict(lm_fit , newdata = College.test , type="terms")
lm_predictions
lm_fit
summary(pcr.fit)
pcr.fit$loadings
pcr.fit$scores
pcr.fit$projection
?pcr
library(tidyverse)
library(ISLR)
library(leaps)# for subset selection
library(glmnet) # for ridge and lasso regression
library(pls) #PCR and PLS regression
#Best subset --------
Hitters <- na.omit(Hitters)
all_subsets <- regsubsets(Salary ~ . , data = Hitters)
summary(all_subsets)
# What it will do is report the details of the best training model for each model SIZE
#default is max size = 8 so we just pass a new max
all_subsets <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19)
summary_of_subsets <- summary(all_subsets)
summary_of_subsets$cp
#Let's plot this information to see which model we shold try
#(remember how Cp is an unbiased estimator for TEST MSE if the estimation of noise variance is unbiased!)
par(mfrow =c(2,2))
plot(summary_of_subsets$rss , xlab ="No of Variables", ylab="RSS" , type ="l")
plot(summary_of_subsets$adjr2 , xlab ="No of Variables", ylab="Adjusted R-squared" , type ="l")
#The points command works like plot except it plots point on an already existing plot
#instead of creating a new plot
#identify point with highest adjusted r squared
which.max(summary_of_subsets$adjr2 )
points(x = 11 , summary_of_subsets$adjr2[11] , col="red" , cex = 2 , pch = 20)
plot(summary_of_subsets$cp , xlab ="No of Variables", ylab="Cp" , type ="l")
points(which.min(summary_of_subsets$cp) ,summary_of_subsets$cp[which.min(summary_of_subsets$cp)] , col="red", cex = 2 , pch=20 )
plot(summary_of_subsets$bic , xlab ="No of Variables", ylab="BIC" , type ="l")
points(which.min(summary_of_subsets$bic) ,summary_of_subsets$bic[which.min(summary_of_subsets$cp)] , col="red", cex = 2 , pch=20 )
par(mfrow=c(1,1))
#also the package has a plotting function
#Change scale argument to change what is on Y-axis. Look at ?plot.regsubsets
plot(all_subsets , scale ="r2")
#Look at linear coefficients of Model with lowest bic
coef(all_subsets , 6)
#Forward/Backward  set selection -----
regfit_forward <- regsubsets(Salary~. , data = Hitters , nvmax=19 ,method = "forward")
summary(regfit_forward)
regfit_backward <- regsubsets(Salary~. , data = Hitters , nvmax=19 ,method = "backward")
summary(regfit_forward)
#How to select best set?
#Validation Set Approach------
#remember, we do subset selecction on TRAINING data, this is because we need test data
#to confirm the appropriatness. We wouldn't get a good estimate of test MSE
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters) , replace = TRUE)
test <- !train
regfit_best <- regsubsets(Salary ~ . , data = Hitters[train,] , nvmax = 19)
#We can look at "best" models for each size by RSS
#Now we create a MODEL MATRIX to be used as a data matrix for the test data
test_mat <- model.matrix(Salary ~ . , data = Hitters[test,])
test_mse <- c(numeric(19))
for( i in 1:19){
coeffi <- coef(regfit_best,id = i)
prediction <- test_mat[,names(coeffi)]%*%coeffi  #multiply data matrix by extracted coefficients
test_mse[i] <- mean((Hitters$Salary[test] - prediction)^2)
}
#View errors
test_mse
#find mind
which.min(test_mse)
#Model 10 minimises test mse
#let's look at the model specification
coef(regfit_best, id = 10)
#Little tedious because there is no predict function within the leaps package
#Let's write out own one
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object , id =id)
xvars <- names(coefi)
#return this now
mat[ , xvars]%*%coefi
}
#Only use test and training split when you are trying to figure out which model size /class to use
# Once you have decided that you will use model of size 10, use ALL data to create those models
#This is because you have already validated it and determined that model size 10 is the most accurate
best_model <- regsubsets(Salary ~ . , data = Hitters , nvmax =19) %>% coef(. ,id=10)
#K-Fold cross Validation Approach---------
#Little more complicated. Need to perform best subset selection within each K- fold
k <- 10
set.seed(1)
folds <- sample(1:k , nrow(Hitters), replace = TRUE)
cv_errors <- matrix(NA,k,19 , dimnames = list(NULL,paste(1:19)))
#rememnber with k fold , kth fold is test set the other k-1 folds are for training!
#also we will be using our predict function written previously predict.regsubsets
for (j in 1:k){
best_fit <- regsubsets( Salary ~ . , data = Hitters[folds!=j,] , nvmax =19)
#within each fold, calculate MSE
for (i in 1:19){
pred <- predict(best_fit, Hitters[folds==j,] , id =i) #perform predictions for each model size on test data (the kth fold)
cv_errors[j,i] <- mean((Hitters$Salary[folds==j] - pred)^2)
}
}
#Now lets calculate average MSE across all k folds to estimate test MSE
cv_averages <- apply(cv_errors,2,mean)
par(mfrow=c(1,1))
plot(cv_averages,type = 'b')
#LOWEST POINTS IS AT 11 variable size
#Therefore, we perform on ALL data, regsubsets to determine what the EXACT size 11 model will look like
coef_size_11 <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19) %>% coef(. , id = 11)
#Shrinkage Method: Ridge Regression -----
grid <- 10^seq(10,-2,length=100)
#glmnet requries model matrix for X values and a vector for y
x <- model.matrix(Salary~ . , data = Hitters)[,-1]
y <- Hitters$Salary
ridge_regression <- glmnet(x,y,alpha=0, lambda = grid)
#Ridge regression includes ALL coefficients, but will shrink noisy ones to near zero (very small)
#look at example coefficients
coef(ridge_regression)[,50] #50th model fit
#Predict function is quite versatile for glmnet object
#instead of predicted Y values, we can predict the Beta coefficients given a new lambda
#predict will INTERPOLATE (i.e. make an educated guess) as to the coefficients if that lambda hasn't explicitly been used
#therefore if you want exact coefficients, set exact = TRUE , glmnet will need to RETRAIN all of those models with the new lambda value inluded
predict(ridge_regression,s=50 ,type="coefficients")[1:20,]
set.seed(1)
train <- sample(c(TRUE,FALSE),nrow(x), replace =TRUE)
test <- !train
test_data <- y[test]
ridge_regression <- glmnet(x = x[train,] , y = y[train] , alpha = 0 , lambda = grid , thresh = 1e-12)
ridge_prediction <- predict(ridge_regression, s =4 , newx = x[test,])
test_mse <- mean((ridge_prediction-test_data)^2)
#Use cross validation to choose best Lambda
set.seed(1)
#REMEMBER ALWAYS use TRAINING DATA for model selection. Therefore allows us to see how useful it really is
#when we estimate the test mse
#training data contains our training and validation sets. Test data contains our test set!
cv.out <- cv.glmnet(x[train,],y[train],alpha =0)
plot(cv.out)
best_lambda <- cv.out$lambda.min
best_lambda
#We can make predictions based off of this best lambda , if newx arguent is
#passed, automattical predicts Y
prediction <- predict(ridge_regression, s= best_lambda , newx = x[test,])
mean((prediction - test_data)^2)
#Shrinkage Method: Lasso Regression ---------
# We can see that the choice of lambda is quite important
lasso_regression <- glmnet(x[train,],y[train], alpha = 1 , lambda = grid)
plot(lasso_regression)
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train], alpha = 1)
plot(cv.out)
best_lambda <- cvs.out$lambda.min
best_lambda
lasso_prediction <- predict(lasso_regression , s= best_lambda , newx = x[test,] )
cat("MSE is now ", mean((lasso_prediction - test_data)^2))
#one advantage of lasso over ridge is that it will give less complicated models
final_models <- glmnet(x,y,alpha=1,lambda = grid)
lasso_coefficients <- predict(final_models , type = "coefficients" , s =best_lambda)
lasso_coefficients[1:20,]
#Dimension Reduction: Principal Components Regression--------
set.seed(2)
pcr_fit <- pcr(Salary ~ . , data = Hitters , scale= T , validation = "CV")
summary(pcr_fit)
#Outputs MSE for all K folds for each no of components used
validationplot(pcr_fit, val.type = "MSEP")
#Huge improvement when 1 component is used (is 0 OLS????)
summary(pcr_fit)
coefplot(pcr_fit)
pcr_fit$projection
?coefplot
coefficients(pcr_fit)
loadings(pcr_fit)
pcr_fit$coefficients
library(tidyverse)
library(ISLR)
install.packages(c("backports", "broom", "caret", "checkpoint", "cowplot", "curl", "dbplyr", "digest", "DRR", "foreach", "fst", "geosphere", "hms", "iterators", "knitr", "officer", "openssl", "quantmod", "raster", "Rcpp", "RcppArmadillo", "recipes", "reshape2", "rgdal", "rlang", "rmarkdown", "rprojroot", "rvg", "shinyWidgets", "sp", "stringi", "tibble", "tidyselect", "timeDate", "withr", "xts", "yaml"))
#Polynomial Regression and Step Functions ---------------
polynomial_fit <- lm(wage ~ poly(age,4) , data = Wage)
library(tidyverse)
library(ISLR)
#Polynomial Regression and Step Functions ---------------
polynomial_fit <- lm(wage ~ poly(age,4) , data = Wage)
?poly
#exctract coefficients
coefficients(polynomial_fit)
#exctract coefficients
coefficients(summary(polynomial_fit))
poly(age,4)
poly(Wage$age,4)
#We will recreate figure 7.1
agelims <- range(Wage$age)
age_grid <- seq(agelims[1],agelims[2])
age_grid
predictions <- predict(polynomial_fit , newdata = list(age=age_grid), se = TRUE)
predictions
se_bands <- cbind(predictions$fit - 2*predictions$se.fit ,predictions$fit + 2*predictions$se.fit )
#Now lets plot this
par(mfrow(c(1,2)), mar =c(4.5,4.5,1,1) , oma = c(0,0,4,0))
#Now lets plot this
par(mfrow=(c(1,2)), mar =c(4.5,4.5,1,1) , oma = c(0,0,4,0))
plot(data = Wage, age, wage, xlim= agelims , cex =0.5 , col = "darkgrey")
plot(Wage$age, Wage$wage, xlim= agelims , cex =0.5 , col = "darkgrey")
title("Degree -4 Polynomial", outer = T)
lines(age_grid, predictions$fit, lwd=2 , col="blue")
?matlines
matlines(age_grid,se_bands,lwd=1,col='blue',lty=3)
plot(Wage$age, Wage$wage, xlim= agelims , cex =0.5 , col = "red")
library(tidyverse)
library(ISLR)
#Polynomial Regression and Step Functions ---------------
polynomial_fit <- lm(wage ~ poly(age,4) , data = Wage)
#exctract coefficients
coefficients(summary(polynomial_fit))
#We can also choose the basis function based on the raw polynomials set Raw = T
#Remember, if you want to type out the polynomials manually, keep it wihtin I(age^2) as the ^ has a special meaning in the R formula notation
#We will recreate figure 7.1
agelims <- range(Wage$age)
age_grid <- seq(agelims[1],agelims[2])
predictions <- predict(polynomial_fit , newdata = list(age=age_grid), se = TRUE)
se_bands <- cbind(predictions$fit - 2*predictions$se.fit ,predictions$fit + 2*predictions$se.fit )
#Now lets plot this
par(mfrow=(c(1,2)), mar =c(4.5,4.5,1,1) , oma = c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim= agelims , cex =0.5 , col = "red")
title("Degree -4 Polynomial", outer = T)
lines(age_grid, predictions$fit, lwd=2 , col="blue") #remember, the x coordinates are specificed first
matlines(age_grid,se_bands,lwd=1,col='blue',lty=3)
par(mfrow=(c(1,2)), mar =c(4.5,4.5,1,1) , oma = c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim= agelims , cex =0.5 , col = "darkgrey")
title("Degree -4 Polynomial", outer = T)
lines(age_grid, predictions$fit, lwd=2 , col="blue") #remember, the x coordinates are specificed first
matlines(age_grid,se_bands,lwd=1,col='blue',lty=3)
fit_1 <- lm(data = Wage , wage ~ age)
fit_2 <- lm(data = Wage , wage ~ poly(age,2))
fit_3 <- lm(data = Wage , wage ~ poly(age,3))
fit_4 <- lm(data = Wage , wage ~ poly(age,4))
fit_5 <- lm(data = Wage , wage ~ poly(age,5))
anova(fit_1,fit_2,fit_3,fit_4,fit_5)
#Notice this
coefficients(summary(fit_5))
cv.glm
?cv.glm
??cv.glm
library(boot)
fit_1 <- glm(data = Wage , wage ~ age)
fit_2 <- glm(data = Wage , wage ~ poly(age,2))
fit_1 <- glm(data = Wage , wage ~ age)
fit_2 <- glm(data = Wage , wage ~ poly(age,2))
fit_3 <- glm(data = Wage , wage ~ poly(age,3))
fit_4 <- glm(data = Wage , wage ~ poly(age,4))
fit_5 <- glm(data = Wage , wage ~ poly(age,5))
anova(fit_1,fit_2,fit_3,fit_4,fit_5)
anova(fit_1,fit_2,fit_3,fit_4,fit_5 ,test = "F")
#Can confirm anova results using cross-validation
cv.glm(data=Wage,list(fit_1,fit_2,fit_3,fit_4,fit_5))
#Can confirm anova results using cross-validation
cv.glm(data=Wage,c(fit_1,fit_2,fit_3,fit_4,fit_5))
#Can confirm anova results using cross-validation
cv.glm(data=Wage,c(fit_1,fit_2,fit_3,fit_4,fit_5),K = 5)
#Can confirm anova results using cross-validation
cv_results <- cv.glm(data=Wage,c(fit_1,fit_2,fit_3,fit_4,fit_5),K = 5)
cv_results$delta
?cv.glm
cv_results <- map(c(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=5 , data=Wage)
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=5 , data=Wage)
cv_results
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=5 , data=Wage) %>% map(,$delta)
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=5 , data=Wage) %>% map($delta)
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=5 , data=Wage) %>% transpose(~delta)
cv_results
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=10 , data=Wage) %>% transpose(delta)
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=10 , data=Wage) %>% transpose(~delta)
cv_results
?I
glm_fit <- glm(I(wage>250)~poly(age,4),data=Wage, family = "binomial")
glm_fit
#note we use the I function as an indicator function which sets values to 1 when wage>250
glm_predictions <- predict(glm_fit, newdata = list(age=age_grid),se = T)
?predict
glm_predictions
#note we use the I function as an indicator function which sets values to 1 when wage>250
glm_predictions <- predict(glm_fit, newdata = list(age=age_grid),se = T,type="response")
glm_predictions
#standard error bands
se_bands_glm <- cbind(glm_predictions$fit - 2*glm_predictions$se.fit ,glm_predictions$fit + 2*glm_predictions$se.fit )
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,0.2))
plot(Wage$age,I(Wage$wage>250),xlim=agelims,type="n",ylim=c(0,0.2))
points(jitter(Wage$age),I((Wage$wage>250)/5), cex=.5,pch="|",col="darkgrey")
lines(age_grid,glm_predictions,lwd=2,col="blue")
lines(age_grid,glm_predictions$fit,lwd=2,col="blue")
matlines(age_grid,se_bands_glm,lwd=1,col="blue",lty=3)
#Let's create a step function
table(cut(Wage$age,4))
step_lm <- lm(wage~cut(age,4),data = Wage)
step_lm
coefficients(summary(step_lm))
cut(Wage$age,4) %>% class
install.packages("splines")
library(splines)
?bs
#bs stands for basis spline, in other words you specify the basis formulae, forms a cubic spline by default , note not a NATURAL cubic spline
regression_splone <- lm(wage ~bs(age,knots=c(25,40,60)),data = Wage)
#bs stands for basis spline, in other words you specify the basis formulae, forms a cubic spline by default , note not a NATURAL cubic spline
regression_spline <- lm(wage ~bs(age,knots=c(25,40,60)),data = Wage)
summary(regression_spline)
regression_spline
dim(Wage)
spline_prediction <- predict(regression_spline, newdata= list(age=age_grid) , se = T)
spline_prediction
plot(Wage$age,Wage$wage, col ="gray")
bs(age,knots=c(25,40,60))
bs(Wage$age,knots=c(25,40,60))
?bs
tmp <- bs(age,knots=c(25,40,60))
tmp <- bs(Wage$age,knots=c(25,40,60))
View(tmp)
View(Wage$age)
#natural spline
natural_spline <- lm(wage~ns(age,df=4), data = Wage)
natural_spline
natural_predictions <- predict(natural_spline , newdata = list(age=age_grid),se=T)
lines(age_grid,natural_predictions$fit,col="red")
#Smoothing Spline
plot(Wage$age,Wage$wage, col ="gray")
title("natural spline")
regression_spline <- lm(wage ~bs(age,knots=c(25,40,60)),data = Wage)
summary(regression_spline)
spline_prediction <- predict(regression_spline, newdata= list(age=age_grid) , se = T)
plot(Wage$age,Wage$wage, col ="gray")
#natural spline
natural_spline <- lm(wage~ns(age,df=4), data = Wage) #can either specify number of knots or by degrees of FREEDOM
natural_predictions <- predict(natural_spline , newdata = list(age=age_grid),se=T)
title("natural spline")
lines(age_grid,natural_predictions$fit,col="red")
#Smoothing Spline
plot(Wage$age,Wage$wage, col ="gray")
library(tidyverse)
library(ISLR)
library(boot)
library(splines)
#Polynomial Regression and Step Functions ---------------
polynomial_fit <- lm(wage ~ poly(age,4) , data = Wage)
#exctract coefficients
coefficients(summary(polynomial_fit))
#We can also choose the basis function based on the raw polynomials set Raw = T
#Remember, if you want to type out the polynomials manually, keep it wihtin I(age^2) as the ^ has a special meaning in the R formula notation
#We will recreate figure 7.1
agelims <- range(Wage$age)
age_grid <- seq(agelims[1],agelims[2])
predictions <- predict(polynomial_fit , newdata = list(age=age_grid), se = TRUE)
se_bands <- cbind(predictions$fit - 2*predictions$se.fit ,predictions$fit + 2*predictions$se.fit )
#Now lets plot this
par(mfrow=(c(1,2)), mar =c(4.5,4.5,1,1) , oma = c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim= agelims , cex =0.5 , col = "darkgrey")
title("Degree -4 Polynomial", outer = T)
lines(age_grid, predictions$fit, lwd=2 , col="blue") #remember, the x coordinates are specificed first
matlines(age_grid,se_bands,lwd=1,col='blue',lty=3)
#What degree polynomial will be the best?
fit_1 <- glm(data = Wage , wage ~ age)
fit_2 <- glm(data = Wage , wage ~ poly(age,2))
fit_3 <- glm(data = Wage , wage ~ poly(age,3))
fit_4 <- glm(data = Wage , wage ~ poly(age,4))
fit_5 <- glm(data = Wage , wage ~ poly(age,5))
anova(fit_1,fit_2,fit_3,fit_4,fit_5 ,test = "F")
#degree 3 or 4 seems to be the best
#Anova method is robust and will work if you have other, non-age variables in the model
#Notice this
coefficients(summary(fit_5))
#P-values of the t-statistics are the same!!
#in fact, if you square the t values, they will equal the f-values
#Can confirm anova results using cross-validation
cv_results <- map(list(fit_1,fit_2,fit_3,fit_4,fit_5),cv.glm, K=10 , data=Wage) %>% transpose(~delta)
#Remember that glm logistic is a type of linear model, so lets try a polynomial glm
glm_fit <- glm(I(wage>250)~poly(age,4),data=Wage, family = "binomial")
#note we use the I function as an indicator function which sets values to 1 when wage>250
glm_predictions <- predict(glm_fit, newdata = list(age=age_grid),se = T,type="response")
#standard error bands
se_bands_glm <- cbind(glm_predictions$fit - 2*glm_predictions$se.fit ,glm_predictions$fit + 2*glm_predictions$se.fit )
plot(Wage$age,I(Wage$wage>250),xlim=agelims,type="n",ylim=c(0,0.2))
points(jitter(Wage$age),I((Wage$wage>250)/5), cex=.5,pch="|",col="darkgrey") #plot actual data, need to transform it to keep it in the range
lines(age_grid,glm_predictions$fit,lwd=2,col="blue")
matlines(age_grid,se_bands_glm,lwd=1,col="blue",lty=3)
#Let's create a step function
table(cut(Wage$age,4))
step_lm <- lm(wage~cut(age,4),data = Wage)
coefficients(summary(step_lm))
#Notice how cut creates factors or categorical variables
#Regression Splines--------
#bs stands for basis spline, in other words you specify the basis formulae, forms a cubic spline by default , note not a NATURAL cubic spline
regression_spline <- lm(wage ~bs(age,knots=c(25,40,60)),data = Wage)
summary(regression_spline)
spline_prediction <- predict(regression_spline, newdata= list(age=age_grid) , se = T)
plot(Wage$age,Wage$wage, col ="gray")
#natural spline
natural_spline <- lm(wage~ns(age,df=4), data = Wage) #can either specify number of knots or by degrees of FREEDOM
natural_predictions <- predict(natural_spline , newdata = list(age=age_grid),se=T)
title("natural spline")
lines(age_grid,natural_predictions$fit,col="red")
#Smoothing Spline
plot(Wage$age,Wage$wage, col ="gray")
title("Smoothing Spline")
smoothing_specified <- smooth.spline(Wage$age,Wage$wage,df=16)
smoothing_cv_determined <- smooth.spline(Wage$age,Wage$wage, cv = T)
smoothing_cv_determined <- smooth.spline(Wage$age, Wage$wage, cv = TRUE)
smoothing_cv_determined
lines(smoothing_specified,col="green")
lines(smoothing_cv_determined,col="black")
#Local Regresion
plot(Wage$age,Wage$wage, col ="gray")
title("LOESS Spline")
title("LOESS ")
#Local Regresion
plot(Wage$age,Wage$wage, col ="gray")
title("LOESS ")
loess_fit <- loess(wage~age,span=0.2,data=Wage)
loess_fit_2 <- loess(wage~age,span=0.5,data=Wage)
lines(loess_fit,col="red")
install.packages("locfit")
#Local Regresion
plot(Wage$age,Wage$wage, col ="gray")
title("LOESS ")
loess_fit <- loess(wage~age,span=0.2,data=Wage)
loess_fit_2 <- loess(wage~age,span=0.5,data=Wage)
lines(age_grid,predict(loess_fit,newdata=list(age=age_grid)),col="red")
lines(age_grid,predict(loess_fit,newdata=list(age=age_grid)),col="red"))
lines(age_grid,predict(loess_fit,newdata=list(age=age_grid)),col="red")
predict(loess_fit,newdata=list(age=age_grid))
predict.loess
?predict.loess
lines(age_grid,predict(loess_fit,newdata=data_frame(age=age_grid)),col="red")
lines(age_grid,predict(loess_fit_2,newdata=data_frame(age=age_grid)),col="blue")
gam_1 <- lm( wage ~ ns(year,4) + ns(age,5) + education , data = Wage)
gam_1
library(gam)
install.packages("gam")
library(gam)
#Because of the issue stated above, we use the gam library and the gam function instead of the base lm function
gam_2 <- gam(wage~s(year,4) + s(age,5) +education , data = Wage) #s() indicates that we wanta smoothing spline with the desired degrees of freedom
gam_2
summmary(gam_2)
summary(gam_2)
par(mfrow=c(1,3))
plot(gam_2,se=T,col="Blue")
#it invokes plot.gam() by default automatically, we can call plot.gam on the "lm" version of the gam (gam_1)
plot.gam(gam_1,se=T,col="red")
#The smoothing spine for year looks linear, lets do an anova on a few models to determine if it should be a linear function rather than a smoothing spline
gam_noyear <- gam(wage~ s(age,5) +education , data = Wage)
gam_linear_year <- gam(wage~ year + s(age,5) +education , data = Wage)
anova(gam_noyear,gam_linear_year , gam_2 , test = "F")
#let's look at a summary of one of the models
summary(gam_2)
?gam
#You can also add in local regression as a building block to a GAM
gam_local_regression_single <- gam(wage ~s(year,df=4) + lo(age,span=.7)+education , data =Wage)
gam_local_regression_single
plot(gam_local_regression_single, se =T , col ="blue")
#try interactions with local regression
gam_local_regression_interaction <- gam(wage~lo(year,age,span=.5)+ education , data = Wage)
#try interactions with local regression
gam_local_regression_interaction <- gam(wage~lo(year,age,span=.6)+ education , data = Wage)
#try interactions with local regression
gam_local_regression_interaction <- gam(wage~lo(year,age,span=.8)+ education , data = Wage)
#try interactions with local regression
gam_local_regression_interaction <- gam(wage~lo(year,age,span=.5)+ education , data = Wage)
install.packages("akima")
library(akima)
#Use akima package to visualise this
plot(gam_local_regression_interaction)
#Use akima package to visualise this
par(mfrow=c(1,2))
plot(gam_local_regression_interaction)
plot(gam_local_regression_interaction , se =T)
#We can also use gams for LOGISTIC REGRESSION
gam_logistic <- gam(I(wage>250) ~year + s(age,df=5) + education , family ="binomial" ,  data = Wage)
par(mfrow=c(1,3))
plot(gam_logistic ,  se = T , col = "red")
#Hold on, look at error bars for <HS grad, why are they so big?
table(Wage$education, I(Wage$wage>250))
#there are no people earning above that wage in <HS grad, it is screwing up the model and should be excluded
gam_logistic <- gam(I(wage>250) ~year + s(age,df=5) + education , family ="binomial" ,  data = Wage, subset =(education!="1. < HS Grad"))
plot(gam_logistic ,  se = T , col = "purple")
