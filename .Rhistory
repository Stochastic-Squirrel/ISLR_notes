coef(lm(mpg~horsepower, data= data , subset = index))
})
}
boot_fn(Auto , sample(392,392, replace = TRUE))
#Bootstrap this
boot(data= Auto , boot_fn , R = 1000)
#compare this against estimates calculated from formula in the lm function
summary(lm(mpg~horsepower,data=Auto))$coef
#You can see that there is actually quite a bit of a difference in the standard error estimation. Why?
#Standard formulae rely on the sigma^2 estimate (noise variance) being accurate.It is onyl accurate if the model selection
# of it being linear is correct. In fact, there is a non-linear relationship so this estimate is over-inflated. Therefore the bootstrap is more accurate in this case.
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1- 1/n)^n
bootstrap_probs <- data_frame(n=numeric(),probability = numeric())
bootstrap_probs$n <- 1:100000
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1- 1/n)^n
bootstrap_probs <- data_frame(n=1:100000,probability = numeric())
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1- 1/n)^n
bootstrap_probs <- data_frame(n=1:100000,probability = numeric(100000))
bootstrap_probs$probability <- 1 - (1- 1/(bootstrap_probs$n) )^(bootstrap_probs$n)
bootstrap_probs
ggplot(bootstrap_probs,aes(x=n , y=probability)) + geom_point()
ggplot(bootstrap_probs,aes(x=n , y=probability)) + geom_smooth()
ggplot(bootstrap_probs,aes(x=n , y=probability)) + geom_point()
ggplot(bootstrap_probs,aes(x=n , y=probability)) +y_lim(0,1)+ geom_point()
ggplot(bootstrap_probs,aes(x=n , y=probability)) +ylim(0,1)+ geom_point()
library(tidyverse)
library(ISLR)
library(boot)
library(tidyverse)
glm_fit <- glm(data = Default , default ~ income + balance)
glm.fit()
Default
glimpse(Default)
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial')
glm_fit
set.seed(1)
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial')
?sample
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) )
test <- !train
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial' , subset = train)
test <- Default[!train,]
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
?predict
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test )
glm_predictions
test
test <- Default[!train,]
test
Default
train
test <- Default[train, ]
test
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) ) %>% arrange()
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) ) %>% order()
train
test <- Default[-train, ]
#train model on training set
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial' , subset = train)
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test )
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
glm_predictions
Default
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response") %>%
case_when(.x>0.5 ~ "Yes",
TRUE ~ "No")
glm_predictions <- predict(glm_fit , newdata = test , type = "response") %>%
case_when(.>0.5 ~ "Yes",
TRUE ~ "No")
glm_predictions <- predict(glm_fit , newdata = test , type = "response") %>%
case_when(.>= 0.5 ~ "Yes",
TRUE ~ "No")
glm_predictions <-  case_when(glm_predictions>= 0.5 ~ "Yes",
TRUE ~ "No")
test_error_rate <- mean(glm_predictions!= test$default)
validation_mse <- function(){
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) )
test <- Default[-train, ]
#train model on training set
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial' , subset = train)
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
glm_predictions <-  case_when(glm_predictions>= 0.5 ~ "Yes",
TRUE ~ "No")
test_error_rate <- mean(glm_predictions!= test$default)
return(test_error_rate)
}
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse_student <- function(){
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) )
test <- Default[-train, ]
#train model on training set
glm_fit <- glm(data = Default , default ~ income + balance + student, family = 'binomial' , subset = train)
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
glm_predictions <-  case_when(glm_predictions>= 0.5 ~ "Yes",
TRUE ~ "No")
test_error_rate <- mean(glm_predictions!= test$default)
return(test_error_rate)
}
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
#Will use resampling to calculate variability of the beta coefficients of the glm model in the previous question
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial')
#look at values for coefficients
summary(glm_fit)
boot_fn(data , index){
glm_fit <- glm(data = data , default ~ income + balance , family = 'binomial' , subset = index)
return(glm_fit$coefficients)
}
boot(Default, boot_fn , R=1000)
boot_fn <- function(data , index){
glm_fit <- glm(data = data , default ~ income + balance , family = 'binomial' , subset = index)
return(glm_fit$coefficients)
}
boo
boot_fn <- function(data , index){
glm_fit <- glm(data = data , default ~ income + balance , family = 'binomial' , subset = index)
return(glm_fit$coefficients)
}
boot(Default, boot_fn , R=1000)
?boot
boot(Default, boot_fn , R=1000 ,parallel = "snow")
boot_results <- boot(Default, boot_fn , R=1000 ,parallel = "snow")
glm_fit$coefficients
summary(glm_fit)
#Doing LOOCV MANUALLY NOW
glm_fit <- glm(data = Weekly , Direction ~ Lag1 + Lag2)
#Doing LOOCV MANUALLY NOW
glm_fit <- glm(data = Weekly , Direction ~ Lag1 + Lag2 , family = "binomial")
dim(Weekly)[1]-1
glm_fit_less1 <- glm(data = Weekly , Direction ~ Lag1 + Lag2 , family = "binomial" , subset = c(FALSE,rep(TRUE,dim(Weekly)[1]-1)))
#Use the second model to predict the direction or Y variable for the FIRST observation (This observation was not included in training set)
predict(glm_fit_less1 , newdata = Weekly[1,] , type = "response")
glm_fit_less1 <- glm(data = Weekly[-1,] , Direction ~ Lag1 + Lag2 , family = "binomial")
#Use the second model to predict the direction or Y variable for the FIRST observation (This observation was not included in training set)
predict(glm_fit_less1 , newdata = Weekly[1,] , type = "response") > 0.5
Weekly
n <- dim(Weekly)[1]
actual <- case_when(Weekly[5,"Direction"]=="Up" ~ TRUE,
TRUE ~ FALSE)
actual
n <- dim(Weekly)[1]
#define vector to record if there was a prediction error
error <- numeric(n)
for ( i in 1:n){
glm_fit_less1 <- glm(data = Weekly[-i,] , Direction ~ Lag1 + Lag2 , family = "binomial")
prediction <- predict(glm_fit_less1 , newdata = Weekly[i,] , type = "response") > 0.5
actual <- case_when(Weekly[i,"Direction"]=="Up" ~ TRUE,
TRUE ~ FALSE)
if(actual==prediction){
error[i] <- 0
}else{
error[i] <- 1
}
}
mse_loocv <- mean(error)
mse_loocv
sum(error)
#Simulate data
set.seed(1)
x <- rnorm(100)
y = x -2*x^2 + rnorm(100)
y <-  x -2*x^2 + rnorm(100)
ggplot(aes(x=x,y=y))
ggplot(data=c(x,y),aes(x=x,y=y))
plot(y~x)
dat <- data_frame(x=x,y=y)
#Produce LOOCV  errors for polynomial models up to order 4
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
errors <- numeric(n)
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
for (degree in 1:4){
errors <- numeric(n)
for ( i in 1:n){
lm_fit_less1 <- lm(data = dat[-i,] , y ~ poly(x,degree) )
prediction <- predict(lm_fit_less1 , newdata = dat[i,] , type = "response")
errors[i] <- (prediction - dat[i,"y"])^2
}
mse_errors$degree <- mean(errors)
}
mse_errors
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
for (degree in 1:4){
errors <- numeric(n)
for ( i in 1:n){
lm_fit_less1 <- lm(data = dat[-i,] , y ~ poly(x,degree) )
prediction <- predict(lm_fit_less1 , newdata = dat[i,] , type = "response")
errors[i] <- (prediction - dat[i,"y"])^2
}
mse_errors[degree] <- mean(errors)
}
mse_errors
set.seed(200)
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
for (degree in 1:4){
errors <- numeric(n)
for ( i in 1:n){
lm_fit_less1 <- lm(data = dat[-i,] , y ~ poly(x,degree) )
prediction <- predict(lm_fit_less1 , newdata = dat[i,] , type = "response")
errors[i] <- (prediction - dat[i,"y"])^2
}
mse_errors[degree] <- mean(errors)
}
mse_errors
mse_errors <- list()
for (degree in 1:4){
lm_fit <- glm(data = dat , y ~ poly(x,degree) )
mse_errors[degree] <- cv.glm(lm_fit_less1,data = dat )$delta[1]
}
mse_errors
lm_fit <- glm(data = dat , y ~ poly(x,1) )
lm_fit
mse_errors <- list()
for (degree in 1:4){
lm_fit <- glm(data = dat , y ~ poly(x,degree) )
mse_errors[degree] <- cv.glm(lm_fit,data = dat )$delta[1]
}
mse_errors
library(MASS)
Boston$medv
mu_estimate <- mean(Boston$medv)
X_bar_deviation <- sd(Boston$medv) / sqrt(length(Boston$medv))
?boot
boot_fn <- function(data, index){
return(mean(data[index,"medv"]))
}
boot(Boston,boot_fn , R=1000)
?predict
#Prediction of 24.47mpg
#Confidence Interval vs Prediction Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval =c("confidence","prediction"))
library(tidyverse)
library(ISLR)
#Question 8-----------
lm_model <-  lm(data = Auto , mpg ~ horsepower)
summary(lm_model)
#Prediction of 24.47mpg
#Confidence Interval vs Prediction Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval =c("confidence","prediction"))
#Prediction of 24.47mpg
#Confidence Interval vs Prediction Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="prediction")
#Confidence Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="confidence")
summary(lm_model)
predict(lm_model , newdata = data_frame(horsepower = 98)) +2*4.906
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="prediction")
dim(Auto)
predict(lm_model , newdata = data_frame(horsepower = 98)) +2*(4.906/sqrt(392))
#Confidence Interval this is for Y_bar
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="confidence")
#plot results
plot(data = Auto , x = horsepower , y =mpg)
?plot
#plot results
plot(Auto , x = horsepower , y =mpg)
#plot results
plot( x = Auto$horsepower , y =Auto$mpg)
abline(lm_model)
abline(lm_model , col = "red")
#Look at diagnostic plots
plot(lm_model)
install.packages("gvlma")
library(gvlma)
gvlma::gvlma(lm_model)
diagnostics <- gvlma::gvlma(lm_model)
summary(diagnostics)
plot(diagnostics)
?gvlma
#Question 9---------
pairs(Auto)
#Question 9---------
pairs(Auto[,-"name"])
colnames(Auto)
#Question 9---------
pairs(Auto[,-9])
cor(Auto[,-9])
lm_fit_2 <- lm(data = Auto , mpg ~ . -name)
summary(lm_fit_2)
#Remember, a point has to have a mix of high leverage and outlier-ness to be INFLUENTIAL
#Let's look at diagnostic plots
plot(lm_fit_2)
plt(gvlma(lm_fit_2))
plot(gvlma(lm_fit_2))
diagnostics_2 <- (gvlma(lm_fit_2))
plot(diagnostics_2)
#Remember, a point has to have a mix of high leverage and outlier-ness to be INFLUENTIAL
#Let's look at diagnostic plots
plot(lm_fit_2)
plot(lm_fit_2)
diagnostics_2
lm_fit_2_interactions <- lm(data = Auto , mpg ~ (. -name)^8)
lm_fit_2_interactions
lm_fit_2_interactions <- lm(mpg~cylinders*displacement+displacement*weight , data = Auto)
#By looking at correlation matrix, try these combinations
#Also have a look at the notes on formula notation in R
summary(lm_fit_2_interactions)
# We can clearly see non-normal relationship betwen mpg and regressors. Let's try a log transformation
lm_fit_2_log <- lm(log(mpg)~cylinders*displacement+displacement*weight , data = Auto)
library(tidyverse)
library(ISLR)
library(gvlma) #
# We can clearly see non-normal relationship betwen mpg and regressors. Let's try a log transformation
lm_fit_2_log <- lm(log(mpg)~cylinders*displacement+displacement*weight , data = Auto)
summary(lm_fit_2_log)
lm_fit_2_interactions <- lm(mpg~cylinders*displacement+displacement*weight , data = Auto)
#By looking at correlation matrix, try these combinations
#Also have a look at the notes on formula notation in R
summary(lm_fit_2_interactions)
#Is this better than the interactions only model?
anova(lm_fit_2_interactions,lm_fit_2_log)
carseats_model <- lm(data = Carseats , Sales ~ Price + Urban  + US)
summary(carseats_model)
summary(Carseats)
#from coefficients, only price and US seem siginificant
update.formula(carseats_model, Sales ~ Price + US)
carseats_model
#from coefficients, only price and US seem siginificant
carseats_improved <-  lm(data = Carseats , Sales ~ Price   + US)
#Get 95% confidence intervals for coefficients
confint(carseats_improved)
library(boot)
#Compare to bootstrap
boot_fn <- function(data , index){
return({
coef(lm(data = data , Sales ~ Price   + US))
})
}
boot(Carseats, boot_fn , R =1000)
boot_fn <- function(data , index){
return({
coef(lm(data = data , Sales ~ Price   + US , subset= index))
})
}
boot(Carseats, boot_fn , R =1000)
#Get 95% confidence intervals for coefficients
confint(carseats_improved)
#Close results
plot(gvlma(carseats_improved))
#Close results
par("mar")
#Close results
par(mar)
#Close results
par('mar')
par(mar=c(1,1,1,1))
plot(gvlma(carseats_improved))
par(mar=c(1.5,1.5,1.5,1.5))
plot(gvlma(carseats_improved))
?par
#Investigate the t-statistic for individual beta coefficients
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)
dat <- data_frame(x=x,y=y)
interceptless_model <- lm (data = dat , y~x -1)
interceptless_model
summary(interceptless_model)
y_onto_x <- lm (data = dat , y~x -1) #without intercept
summary(y_onto_x)
x_onto_y<- lm (data = dat , x~y -1) #without intercept
summary(x_onto_y)
summary(y_onto_x)
library(tidyverse)
library(ISLR)
library(gvlma) #found this online. Adds additional diagnostic plots
library(boot)
set.seed(1)
x = rnorm(100)
y = 2*x
lm.fit = lm(y~x+0)
lm.fit2 = lm(x~y+0)
summary(lm.fit)
summary(lm.fit2)
set.seed(1)
x <- rnorm(100)
y <- -sample(x, 100)
sum(x^2)
lm.fit <- lm(y~x+0)
lm.fit2 <- lm(x~y+0)
summary(lm.fit)
summary(lm.fit2)
#Same coefficient estimates when the sum of squares are the same
#Question 13 --------
set.seed(1)
x <- rnorm(100)
eps <- rnorm(mean = 0 , sd = sqrt(0.25) , n =100)
y <- -1 + 0.5*x + eps
#True bo would be -1 and b_1 would be 0.5
plot(x,y)
lm_fit <- lm(y~x)
lm_fit
summary(lm_fit)
#Estimated coefficients are pretty close to the true values. This is because true relationship is linear so lm does a good job approximating the true funcitonal form f
abline(lm_fit)
#Estimated coefficients are pretty close to the true values. This is because true relationship is linear so lm does a good job approximating the true funcitonal form f
abline(lm_fit , col =5)
abline(a = -1 , b =0.5 , col =6)
legend(-1, legend = c("model fit", "pop. regression"), col=5:6, lwd=3)
legend(legend = c("model fit", "pop. regression"), col=5:6, lwd=3)
legend(-1,legend = c("model fit", "pop. regression"), col=5:6, lwd=3)
?legend
polynomial_fit <- lm(y ~ poly(x , 2))
summary(polynomial_fit)
#compare against linear model
summary(lm_fit)
anova(lm_fit,polynomial_fit)
eps_low_noise <- rnorm(mean = 0 , sd = sqrt(0.05) , n =100)
eps_low_noise <- rnorm(mean = 0 , sd = sqrt(0.05) , n =100)
y_low_noise <- -1 + 0.5*x + eps_low_noise
eps_high_noise <- rnorm(mean = 0 , sd = sqrt(1.5) , n =100)
y_high_noise <- -1 + 0.5*x + eps_high_noise
plot(x,y , main="Comparison of different LM under different irreducible noise")
abline(lm_fit)
abline(lm_fit_low_noise)
lm_fit_low_noise <- lm(y_low_noise~x)
lm_fit_high_noise <- lm(y_high_noise~x)
abline(lm_fit , col =1)
abline(lm_fit_low_noise , col = 2)
abline(lm_fit_high_noise , col = 3)
abline(a = -1 , b =0.5 , col =6)
legend(-1,legend = c("medium_noise","low_noise","high_noise","population_relation"), col=c(1,2,3,6), lwd=3)
#Let's see how noise affects the CI
models <- c(lm_fit, lm_fit_low_noise , lm_fit_high_noise)
map(models, confint)
#Let's see how noise affects the CI
models <- list(lm_fit, lm_fit_low_noise , lm_fit_high_noise)
map(models, confint)
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
#Y  = 2 +2x1 +.3x2
cor(x1,x2)
plot(x1~x2)
#Very high collinearity
lm_collinear <- lm(y~x1+x2)
summary(lm_collinear)
# x1 significant x2 not signifincat
# estimates are prettyfar off from true population, betas
#This is because the betas are calculated GIVEN that other variables are held constant
#This is why collinearity is a problem
lm_x2 <- lm(y~x2)
lm_x1 <- lm(y~x1)
summary(lm_x2)
summary(lm_x1)
#Question 15 ---------
summary(Boston)
library(MASS)
#Question 15 ---------
summary(Boston)
pairs(Boston)
lm(crime ~ poly(dis,3))
lm(crime ~ poly(dis,3) , data = Boston)
lm(crim ~ poly(dis,3) , data = Boston)
lm(crim ~ poly(dis,3) , data = Boston) %>% summary()
lm(crim ~ poly(dis,4) , data = Boston) %>% summary()
lm(crim ~ poly(dis,8) , data = Boston) %>% summary()
poly <- lm(crim ~ poly(dis,3) , data = Boston)
plot(Boston$dis , Boston$dis)
plot(Boston$dis , Boston$crim)
lines(poly)
line(poly)
ggplot(Boston , aes(x=dis,y=crim)) + geom_point()
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + geom_smooth(poly)
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + stat_smooth(poly)
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + stat_smooth()
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + stat_smooth(formula = crim ~ poly(dis,3) )
