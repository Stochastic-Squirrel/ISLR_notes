library(ISLR)
library(gvlma) #
# We can clearly see non-normal relationship betwen mpg and regressors. Let's try a log transformation
lm_fit_2_log <- lm(log(mpg)~cylinders*displacement+displacement*weight , data = Auto)
summary(lm_fit_2_log)
lm_fit_2_interactions <- lm(mpg~cylinders*displacement+displacement*weight , data = Auto)
#By looking at correlation matrix, try these combinations
#Also have a look at the notes on formula notation in R
summary(lm_fit_2_interactions)
#Is this better than the interactions only model?
anova(lm_fit_2_interactions,lm_fit_2_log)
carseats_model <- lm(data = Carseats , Sales ~ Price + Urban  + US)
summary(carseats_model)
summary(Carseats)
#from coefficients, only price and US seem siginificant
update.formula(carseats_model, Sales ~ Price + US)
carseats_model
#from coefficients, only price and US seem siginificant
carseats_improved <-  lm(data = Carseats , Sales ~ Price   + US)
#Get 95% confidence intervals for coefficients
confint(carseats_improved)
library(boot)
#Compare to bootstrap
boot_fn <- function(data , index){
return({
coef(lm(data = data , Sales ~ Price   + US))
})
}
boot(Carseats, boot_fn , R =1000)
boot_fn <- function(data , index){
return({
coef(lm(data = data , Sales ~ Price   + US , subset= index))
})
}
boot(Carseats, boot_fn , R =1000)
#Get 95% confidence intervals for coefficients
confint(carseats_improved)
#Close results
plot(gvlma(carseats_improved))
#Close results
par("mar")
#Close results
par(mar)
#Close results
par('mar')
par(mar=c(1,1,1,1))
plot(gvlma(carseats_improved))
par(mar=c(1.5,1.5,1.5,1.5))
plot(gvlma(carseats_improved))
?par
#Investigate the t-statistic for individual beta coefficients
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)
dat <- data_frame(x=x,y=y)
interceptless_model <- lm (data = dat , y~x -1)
interceptless_model
summary(interceptless_model)
y_onto_x <- lm (data = dat , y~x -1) #without intercept
summary(y_onto_x)
x_onto_y<- lm (data = dat , x~y -1) #without intercept
summary(x_onto_y)
summary(y_onto_x)
library(tidyverse)
library(ISLR)
library(gvlma) #found this online. Adds additional diagnostic plots
library(boot)
set.seed(1)
x = rnorm(100)
y = 2*x
lm.fit = lm(y~x+0)
lm.fit2 = lm(x~y+0)
summary(lm.fit)
summary(lm.fit2)
set.seed(1)
x <- rnorm(100)
y <- -sample(x, 100)
sum(x^2)
lm.fit <- lm(y~x+0)
lm.fit2 <- lm(x~y+0)
summary(lm.fit)
summary(lm.fit2)
#Same coefficient estimates when the sum of squares are the same
#Question 13 --------
set.seed(1)
x <- rnorm(100)
eps <- rnorm(mean = 0 , sd = sqrt(0.25) , n =100)
y <- -1 + 0.5*x + eps
#True bo would be -1 and b_1 would be 0.5
plot(x,y)
lm_fit <- lm(y~x)
lm_fit
summary(lm_fit)
#Estimated coefficients are pretty close to the true values. This is because true relationship is linear so lm does a good job approximating the true funcitonal form f
abline(lm_fit)
#Estimated coefficients are pretty close to the true values. This is because true relationship is linear so lm does a good job approximating the true funcitonal form f
abline(lm_fit , col =5)
abline(a = -1 , b =0.5 , col =6)
legend(-1, legend = c("model fit", "pop. regression"), col=5:6, lwd=3)
legend(legend = c("model fit", "pop. regression"), col=5:6, lwd=3)
legend(-1,legend = c("model fit", "pop. regression"), col=5:6, lwd=3)
?legend
polynomial_fit <- lm(y ~ poly(x , 2))
summary(polynomial_fit)
#compare against linear model
summary(lm_fit)
anova(lm_fit,polynomial_fit)
eps_low_noise <- rnorm(mean = 0 , sd = sqrt(0.05) , n =100)
eps_low_noise <- rnorm(mean = 0 , sd = sqrt(0.05) , n =100)
y_low_noise <- -1 + 0.5*x + eps_low_noise
eps_high_noise <- rnorm(mean = 0 , sd = sqrt(1.5) , n =100)
y_high_noise <- -1 + 0.5*x + eps_high_noise
plot(x,y , main="Comparison of different LM under different irreducible noise")
abline(lm_fit)
abline(lm_fit_low_noise)
lm_fit_low_noise <- lm(y_low_noise~x)
lm_fit_high_noise <- lm(y_high_noise~x)
abline(lm_fit , col =1)
abline(lm_fit_low_noise , col = 2)
abline(lm_fit_high_noise , col = 3)
abline(a = -1 , b =0.5 , col =6)
legend(-1,legend = c("medium_noise","low_noise","high_noise","population_relation"), col=c(1,2,3,6), lwd=3)
#Let's see how noise affects the CI
models <- c(lm_fit, lm_fit_low_noise , lm_fit_high_noise)
map(models, confint)
#Let's see how noise affects the CI
models <- list(lm_fit, lm_fit_low_noise , lm_fit_high_noise)
map(models, confint)
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
#Y  = 2 +2x1 +.3x2
cor(x1,x2)
plot(x1~x2)
#Very high collinearity
lm_collinear <- lm(y~x1+x2)
summary(lm_collinear)
# x1 significant x2 not signifincat
# estimates are prettyfar off from true population, betas
#This is because the betas are calculated GIVEN that other variables are held constant
#This is why collinearity is a problem
lm_x2 <- lm(y~x2)
lm_x1 <- lm(y~x1)
summary(lm_x2)
summary(lm_x1)
#Question 15 ---------
summary(Boston)
library(MASS)
#Question 15 ---------
summary(Boston)
pairs(Boston)
lm(crime ~ poly(dis,3))
lm(crime ~ poly(dis,3) , data = Boston)
lm(crim ~ poly(dis,3) , data = Boston)
lm(crim ~ poly(dis,3) , data = Boston) %>% summary()
lm(crim ~ poly(dis,4) , data = Boston) %>% summary()
lm(crim ~ poly(dis,8) , data = Boston) %>% summary()
poly <- lm(crim ~ poly(dis,3) , data = Boston)
plot(Boston$dis , Boston$dis)
plot(Boston$dis , Boston$crim)
lines(poly)
line(poly)
ggplot(Boston , aes(x=dis,y=crim)) + geom_point()
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + geom_smooth(poly)
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + stat_smooth(poly)
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + stat_smooth()
ggplot(Boston , aes(x=dis,y=crim)) + geom_point() + stat_smooth(formula = crim ~ poly(dis,3) )
demo()
set.seed(1)
library(tidyverse)
library(ISLR)
library(leaps)# for subset selection
#Best subset --------
Hitters <- na.omit(Hitters)
all_subsets <- regsubsets(Salary ~ . , data = Hitters)
summary(all_subsets)
# What it will do is report the details of the best training model for each model SIZE
#default is max size = 8 so we just pass a new max
all_subsets <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19)
summary_of_subsets <- summary(all_subsets)
summary_of_subsets$cp
#Let's plot this information to see which model we shold try
#(remember how Cp is an unbiased estimator for TEST MSE if the estimation of noise variance is unbiased!)
par(mfrow =c(2,2))
plot(summary_of_subsets$rss , xlab ="No of Variables", ylab="RSS" , type ="l")
plot(summary_of_subsets$adjr2 , xlab ="No of Variables", ylab="Adjusted R-squared" , type ="l")
#The points command works like plot except it plots point on an already existing plot
#instead of creating a new plot
#identify point with highest adjusted r squared
which.max(summary_of_subsets$adjr2 )
points(x = 11 , summary_of_subsets$adjr2[11] , col="red" , cex = 2 , pch = 20)
plot(summary_of_subsets$cp , xlab ="No of Variables", ylab="Cp" , type ="l")
points(which.min(summary_of_subsets$cp) ,summary_of_subsets$cp[which.min(summary_of_subsets$cp)] , col="red", cex = 2 , pch=20 )
plot(summary_of_subsets$bic , xlab ="No of Variables", ylab="BIC" , type ="l")
points(which.min(summary_of_subsets$bic) ,summary_of_subsets$bic[which.min(summary_of_subsets$cp)] , col="red", cex = 2 , pch=20 )
par(mfrow=c(1,1))
#also the package has a plotting function
#Change scale argument to change what is on Y-axis. Look at ?plot.regsubsets
plot(all_subsets , scale ="r2")
#Look at linear coefficients of Model with lowest bic
coef(all_subsets , 6)
#Forward/Backward  set selection -----
regfit_forward <- regsubsets(Salary~. , data = Hitters , nvmax=19 ,method = "forward")
summary(regfit_forward)
regfit_backward <- regsubsets(Salary~. , data = Hitters , nvmax=19 ,method = "backward")
summary(regfit_forward)
#How to select best set? ---------
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters) , replace = TRUE)
test <- !train
regfit_best <- regsubsets(Salary ~ . , data = Hitters[train,] , nvmax = 19)
#We can look at "best" models for each size by RSS
#Now we create a MODEL MATRIX to be used as a data matrix for the test data
test_mat <- model.matrix(Salary ~ . , data = Hitters[test,])
test_mat
test_mse <- c(numeric(19))
for( i in 1:19){
coeffi <- coef(regfit_best,id = i)
prediction <- test_mat[,names(coeffi)]%*%coefi  #multiply data matrix by extracted coefficients
test_mse[i] <- mean((Hitters$Salary[test] - prediction)^2)
}
for( i in 1:19){
coeffi <- coef(regfit_best,id = i)
prediction <- test_mat[,names(coeffi)]%*%coeffi  #multiply data matrix by extracted coefficients
test_mse[i] <- mean((Hitters$Salary[test] - prediction)^2)
}
#View errors
test_mse
#find mind
which.min(test_mse)
#Model 10 minimises test mse
#let's look at the model specification
coef(regfit_best, id = 10)
best_model <- regsubsets(Salary ~ . , data = Hitters , nvmax =19)
best_model <- regsubsets(Salary ~ . , data = Hitters , nvmax =19) %>% coef(. ,id=10)
best_model
#Little more complicated. Need to perform best subset selection within each K- fold
k <- 10
set.seed(1)
folds <- sample(1:k , nrow(Hitters), replace = TRUE)
cv_errors <- matrix(NA,k,19 , dimnames = list(NULL,paste(1:19)))
View(cv_errors)
k <- 10
set.seed(1)
folds <- sample(1:k , nrow(Hitters), replace = TRUE)
cv_errors <- matrix(NA,k,19 , dimnames = list(NULL,paste(1:19)))
#rememnber with k fold , kth fold is test set the other k-1 folds are for training!
#also we will be using our predict function written previously predict.regsubsets
for (j in 1:k){
best_fit <- regsubsets( Salary ~ . , data = Hitters[folds!=j] , nvmax =19)
#within each fold, calculate MSE
for (i in 1:19){
pred <- predict(best_fit, Hitters[folds==j,] , id =i) #perform predictions for each model size on test data (the kth fold)
cv_errors[j,i] <- mean((Hitters$Salary[folds==j] - pred)^2)
}
}
k <- 10
set.seed(1)
folds <- sample(1:k , nrow(Hitters), replace = TRUE)
cv_errors <- matrix(NA,k,19 , dimnames = list(NULL,paste(1:19)))
#rememnber with k fold , kth fold is test set the other k-1 folds are for training!
#also we will be using our predict function written previously predict.regsubsets
for (j in 1:k){
best_fit <- regsubsets( Salary ~ . , data = Hitters[folds!=j,] , nvmax =19)
#within each fold, calculate MSE
for (i in 1:19){
pred <- predict(best_fit, Hitters[folds==j,] , id =i) #perform predictions for each model size on test data (the kth fold)
cv_errors[j,i] <- mean((Hitters$Salary[folds==j] - pred)^2)
}
}
#Validation Set Approach------
#remember, we do subset selecction on TRAINING data, this is because we need test data
#to confirm the appropriatness. We wouldn't get a good estimate of test MSE
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters) , replace = TRUE)
test <- !train
regfit_best <- regsubsets(Salary ~ . , data = Hitters[train,] , nvmax = 19)
#We can look at "best" models for each size by RSS
#Now we create a MODEL MATRIX to be used as a data matrix for the test data
test_mat <- model.matrix(Salary ~ . , data = Hitters[test,])
test_mse <- c(numeric(19))
for( i in 1:19){
coeffi <- coef(regfit_best,id = i)
prediction <- test_mat[,names(coeffi)]%*%coeffi  #multiply data matrix by extracted coefficients
test_mse[i] <- mean((Hitters$Salary[test] - prediction)^2)
}
#View errors
test_mse
#find mind
which.min(test_mse)
#Model 10 minimises test mse
#let's look at the model specification
coef(regfit_best, id = 10)
#Little tedious because there is no predict function within the leaps package
#Let's write out own one
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object , id =id)
xvars <- names(coefi)
#return this now
mat[ , xvars]%*%coefi
}
#Only use test and training split when you are trying to figure out which model size /class to use
# Once you have decided that you will use model of size 10, use ALL data to create those models
#This is because you have already validated it and determined that model size 10 is the most accurate
best_model <- regsubsets(Salary ~ . , data = Hitters , nvmax =19) %>% coef(. ,id=10)
k <- 10
set.seed(1)
folds <- sample(1:k , nrow(Hitters), replace = TRUE)
cv_errors <- matrix(NA,k,19 , dimnames = list(NULL,paste(1:19)))
#rememnber with k fold , kth fold is test set the other k-1 folds are for training!
#also we will be using our predict function written previously predict.regsubsets
for (j in 1:k){
best_fit <- regsubsets( Salary ~ . , data = Hitters[folds!=j,] , nvmax =19)
#within each fold, calculate MSE
for (i in 1:19){
pred <- predict(best_fit, Hitters[folds==j,] , id =i) #perform predictions for each model size on test data (the kth fold)
cv_errors[j,i] <- mean((Hitters$Salary[folds==j] - pred)^2)
}
}
cv_averages <- apply(cv_errors,2,mean)
cv_averages
par(mfrow=c(1,1))
plot(cv_averages,type = 'b')
coef_size_11 <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19)
coef_size_11 <- regsubsets %>% coef(. , id =11)
coef_size_11 <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19) %>% coef(. , id = 11)
coef_size_11
sqrt(125)
sqrt(12500)
Hitters$Salary
library(glmnet)
library(tidyverse)
library(ISLR)
library(leaps)# for subset selection
library(glmnet) # for ridge and lasso regression
#Best subset --------
Hitters <- na.omit(Hitters)
all_subsets <- regsubsets(Salary ~ . , data = Hitters)
summary(all_subsets)
# What it will do is report the details of the best training model for each model SIZE
#default is max size = 8 so we just pass a new max
all_subsets <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19)
summary_of_subsets <- summary(all_subsets)
summary_of_subsets$cp
#Let's plot this information to see which model we shold try
#(remember how Cp is an unbiased estimator for TEST MSE if the estimation of noise variance is unbiased!)
par(mfrow =c(2,2))
plot(summary_of_subsets$rss , xlab ="No of Variables", ylab="RSS" , type ="l")
plot(summary_of_subsets$adjr2 , xlab ="No of Variables", ylab="Adjusted R-squared" , type ="l")
#The points command works like plot except it plots point on an already existing plot
#instead of creating a new plot
#identify point with highest adjusted r squared
which.max(summary_of_subsets$adjr2 )
points(x = 11 , summary_of_subsets$adjr2[11] , col="red" , cex = 2 , pch = 20)
plot(summary_of_subsets$cp , xlab ="No of Variables", ylab="Cp" , type ="l")
points(which.min(summary_of_subsets$cp) ,summary_of_subsets$cp[which.min(summary_of_subsets$cp)] , col="red", cex = 2 , pch=20 )
plot(summary_of_subsets$bic , xlab ="No of Variables", ylab="BIC" , type ="l")
points(which.min(summary_of_subsets$bic) ,summary_of_subsets$bic[which.min(summary_of_subsets$cp)] , col="red", cex = 2 , pch=20 )
par(mfrow=c(1,1))
#also the package has a plotting function
#Change scale argument to change what is on Y-axis. Look at ?plot.regsubsets
plot(all_subsets , scale ="r2")
#Look at linear coefficients of Model with lowest bic
coef(all_subsets , 6)
#Forward/Backward  set selection -----
regfit_forward <- regsubsets(Salary~. , data = Hitters , nvmax=19 ,method = "forward")
summary(regfit_forward)
regfit_backward <- regsubsets(Salary~. , data = Hitters , nvmax=19 ,method = "backward")
summary(regfit_forward)
#How to select best set?
#Validation Set Approach------
#remember, we do subset selecction on TRAINING data, this is because we need test data
#to confirm the appropriatness. We wouldn't get a good estimate of test MSE
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters) , replace = TRUE)
test <- !train
regfit_best <- regsubsets(Salary ~ . , data = Hitters[train,] , nvmax = 19)
#We can look at "best" models for each size by RSS
#Now we create a MODEL MATRIX to be used as a data matrix for the test data
test_mat <- model.matrix(Salary ~ . , data = Hitters[test,])
test_mse <- c(numeric(19))
for( i in 1:19){
coeffi <- coef(regfit_best,id = i)
prediction <- test_mat[,names(coeffi)]%*%coeffi  #multiply data matrix by extracted coefficients
test_mse[i] <- mean((Hitters$Salary[test] - prediction)^2)
}
#View errors
test_mse
#find mind
which.min(test_mse)
#Model 10 minimises test mse
#let's look at the model specification
coef(regfit_best, id = 10)
#Little tedious because there is no predict function within the leaps package
#Let's write out own one
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object , id =id)
xvars <- names(coefi)
#return this now
mat[ , xvars]%*%coefi
}
#Only use test and training split when you are trying to figure out which model size /class to use
# Once you have decided that you will use model of size 10, use ALL data to create those models
#This is because you have already validated it and determined that model size 10 is the most accurate
best_model <- regsubsets(Salary ~ . , data = Hitters , nvmax =19) %>% coef(. ,id=10)
#K-Fold cross Validation Approach---------
#Little more complicated. Need to perform best subset selection within each K- fold
k <- 10
set.seed(1)
folds <- sample(1:k , nrow(Hitters), replace = TRUE)
cv_errors <- matrix(NA,k,19 , dimnames = list(NULL,paste(1:19)))
#rememnber with k fold , kth fold is test set the other k-1 folds are for training!
#also we will be using our predict function written previously predict.regsubsets
for (j in 1:k){
best_fit <- regsubsets( Salary ~ . , data = Hitters[folds!=j,] , nvmax =19)
#within each fold, calculate MSE
for (i in 1:19){
pred <- predict(best_fit, Hitters[folds==j,] , id =i) #perform predictions for each model size on test data (the kth fold)
cv_errors[j,i] <- mean((Hitters$Salary[folds==j] - pred)^2)
}
}
#Now lets calculate average MSE across all k folds to estimate test MSE
cv_averages <- apply(cv_errors,2,mean)
par(mfrow=c(1,1))
plot(cv_averages,type = 'b')
#LOWEST POINTS IS AT 11 variable size
#Therefore, we perform on ALL data, regsubsets to determine what the EXACT size 11 model will look like
coef_size_11 <- regsubsets(Salary ~ . , data = Hitters , nvmax = 19) %>% coef(. , id = 11)
#Shrinkage Method: Ridge Regression -----
grid <- 10^seq(10,-2,length-.=100)
#Shrinkage Method: Ridge Regression -----
grid <- 10^seq(10,-2,length=100)
grid
ridge_regression <- glmnet(x,y,alpha=0, lambda = grid)
#glmnet requries model matrix for X values and a vector for y
x <- model.matrix(Salary~.)[,-1]
#glmnet requries model matrix for X values and a vector for y
x <- model.matrix(Salary~ . , data = Hitters)[,-1]
model.matrix(Salary~ . , data = Hitters
)
ridge_regression <- glmnet(x,y,alpha=0, lambda = grid)
y <- Hitters$Salary
ridge_regression <- glmnet(x,y,alpha=0, lambda = grid)
ridge_regression
#Ridge regression includes ALL coefficients, but will shrink noisy ones to near zero (very small)
#look at example coefficients
coef(ridge_regression)
#Ridge regression includes ALL coefficients, but will shrink noisy ones to near zero (very small)
#look at example coefficients
coef(ridge_regression)[,50]
?predict.glmnet
#Predict function is quite versatile for glmnet object
#instead of predicted Y values, we can predict the Beta coefficients given a new lambda
predic(ridge_regression,s=50 ,type="coefficients")
#Predict function is quite versatile for glmnet object
#instead of predicted Y values, we can predict the Beta coefficients given a new lambda
predict(ridge_regression,s=50 ,type="coefficients")
#Predict function is quite versatile for glmnet object
#instead of predicted Y values, we can predict the Beta coefficients given a new lambda
predict(ridge_regression,s=50 ,type="coefficients")[1:20,]
set.seed(1)
train <- sample(c(TRUE,FALSE),1:nrow(x), replace =TRUE)
test <- !train
test_data <- y[test]
class(grid)
ridge_regression <- glmnet(x = x[train,] , y = y[train] , alpha = 0 , lambda = grid , thresh = 1e-12)
ridge_prediction <- predict(ridge_regression, s =4 , newx = x[test,])
ridge_prediction
x[test,]
test
train
train <- sample(c(TRUE,FALSE),nrow(x), replace =TRUE)
test <- !train
test_data <- y[test]
ridge_regression <- glmnet(x = x[train,] , y = y[train] , alpha = 0 , lambda = grid , thresh = 1e-12)
ridge_prediction <- predict(ridge_regression, s =4 , newx = x[test,])
ridge_prediction
ridge_prediction <- predict(ridge_regression, s =4 , newx = x[test,])
test_mse <- mean((ridge_prediction-test_data)^2)
test_mse
set.seed(1)
train <- sample(c(TRUE,FALSE),nrow(x), replace =TRUE)
test <- !train
test_data <- y[test]
ridge_regression <- glmnet(x = x[train,] , y = y[train] , alpha = 0 , lambda = grid , thresh = 1e-12)
ridge_prediction <- predict(ridge_regression, s =4 , newx = x[test,])
test_mse <- mean((ridge_prediction-test_data)^2)
test_mse
ridge_prediction %>% names
View(ridge_prediction)
View(Hitters)
#Use cross validation to choose best Lambda
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha =0)
cv.out
cv.out$lambda
plot(cv.out)
best_lambda <- cv.out$lambda.min
480/6
87+88+92+95+90
452/5
best_lambda
?cv.glmnet
#We can make predictions based off of this best lambda
prediction <- predict(ridge_regression, s= best_lambda , newx = x[test,])
prediction
mean((prediction - test_data)^2)
