for (i in 1:10){
glm_fit <- glm(mpg~poly(horsepower,i) , data = Auto)
cv_error[i] <- cv.glm(data = Auto , glm_fit, K = 10)$delta[1]
}
cv_error
alpha_fn <- function( data , index){
X <- data$X[index]
Y <- data$Y[index]
return((var(Y)- cov(X,Y)) / (var(X)+var(Y)-2*cov(X,Y)) )
}
#estimates allpha value or a portfolio allocation percentage to be optimal
set.seed(1)
alpha_fn(Portfolio,sample(100,100,replace = T))
#Now let's bootstrap the above process in order to get a robust CI for alpha
boot(Portfolio , alpha_fn , R=1000)
boot_fn <- function(data , index){
retrun({
coef(lm(mpg~horsepower, data= data , subset = index))
})
}
boot_fn(Auto , sample(392,392, replace = TRUE))
boot_fn <- function(data , index){
return({
coef(lm(mpg~horsepower, data= data , subset = index))
})
}
boot_fn(Auto , sample(392,392, replace = TRUE))
?boot
#Bootstrap this
boot(data= Auto , boot_fn , R = 1000)
#compare this against estimates calculated from formula in the lm function
summary(lm(mpg~horsepower,data=Auto))$coef
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1/n)^n
boostrap_probs <- data_frame(n=numeric(),probability = numeric())
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1/n)^n
bootsrap_probs <- data_frame(n=numeric(),probability = numeric())
bootstrap_probs
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1/n)^n
bootstrap_probs <- data_frame(n=numeric(),probability = numeric())
bootstrap_probs
bootstrap_probs$probability <- 1 - (1- 1/(1:100000) )^(1:100000)
1 - (1- 1/(1:100000) )^(1:100000)
bootstrap_probs$probability <- 1 - (1- 1/(bootstrap_probs$n) )^(bootstrap_probs$n)
View(bootstrap_probs)
library(tidyverse)
library(ISLR)
library(boot)
set.seed(1)
#THE VALIDATION SET APPROACH
#sample without replacement from numbers 1:392
train <- sample(392,196)
lm_model <- lm(mpg~horsepower, data = Auto , subset = train)
#Calculate MSE = average of (Actual - predicted)^2
mse_lm_model <-   (Auto[-train,"mpg"] - predict(lm_model , newdata=Auto[-train,]))^2 %>% mean()
#Calculate MSE of polynomial regression
poly2_model <- lm(mpg~poly(horsepower,2), data = Auto , subset = train)
mse_poly2_model <-   (Auto[-train,"mpg"] - predict(poly2_model , newdata=Auto[-train,]))^2 %>% mean()
poly3_model <- lm(mpg~poly(horsepower,3), data = Auto , subset = train)
mse_poly3_model <-   (Auto[-train,"mpg"] - predict(poly3_model , newdata=Auto[-train,]))^2 %>% mean()
#NOTE. It is advisable to use Poly(x , power) rather than I(x^power) or x^power when specifying the model formula
#Read notes on orthogonal polynomials for regression
#LEAVE OUT ONE CROSS VALIDATION (LOOCV)
#note lm ~ glm if no family argument is passed to glm
glm_fit <- glm(mpg~horsepower , data = Auto)
cv_error <-  cv.glm(data = Auto , glm_fit)
#cv.glm allows you to calculate cross validation MSE
# cverror$delta contains the estimated test MSE which is the average of errors for each prediction
#We will explore a case later where those two numbers will differ
cv_error <- numeric(5)
for (i in 1:5){
glm_fit <- glm(mpg~poly(horsepower,i) , data = Auto)
cv_error[i] <- cv.glm(data = Auto , glm_fit)$delta[1]
}
#NOTE. MSE estimates will ALWAYS be the same because of LOOCV, you are always going to be testing the same
#models against the same points. Therefore no variability in estimates
#K-FOLD CROSS VALIDATION
# cv.glm can also do K-fold, if you don't set the k parameter, it will set K=N , which is LOOCV
set.seed(17)
cv_error <- numeric(10)
for (i in 1:10){
glm_fit <- glm(mpg~poly(horsepower,i) , data = Auto)
cv_error[i] <- cv.glm(data = Auto , glm_fit, K = 10)$delta[1]
}
#Much faster computation than LOOCV, also has the perks of a more accurate estimate of MSE
# even though it will have slightly more bias, it reduces variance because instead of averaging MSE_i's of one point each
# as in LOOCV , you average MSE_k's which in itself, is an average, so it is an average of averages.
#Note. the two delta values differe slightly, first one is the standard K-fold MSE estimate, the other is a bias corrected estimate
#BOOTSTRAP
alpha_fn <- function( data , index){
X <- data$X[index]
Y <- data$Y[index]
return((var(Y)- cov(X,Y)) / (var(X)+var(Y)-2*cov(X,Y)) )
}
#estimates allpha value or a portfolio allocation percentage to be optimal
set.seed(1)
alpha_fn(Portfolio,sample(100,100,replace = T))
#Now let's bootstrap the above process in order to get a robust CI for alpha
boot(Portfolio , alpha_fn , R=1000)
#SHows SE(alpha) = 0.0886 and estimate for alpha is 0.57583
#Let's apply this thinking to linear regression coefficient estimates compared to the analytical formulae for OLS estimation
boot_fn <- function(data , index){
return({
coef(lm(mpg~horsepower, data= data , subset = index))
})
}
boot_fn(Auto , sample(392,392, replace = TRUE))
#Bootstrap this
boot(data= Auto , boot_fn , R = 1000)
#compare this against estimates calculated from formula in the lm function
summary(lm(mpg~horsepower,data=Auto))$coef
#You can see that there is actually quite a bit of a difference in the standard error estimation. Why?
#Standard formulae rely on the sigma^2 estimate (noise variance) being accurate.It is onyl accurate if the model selection
# of it being linear is correct. In fact, there is a non-linear relationship so this estimate is over-inflated. Therefore the bootstrap is more accurate in this case.
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1- 1/n)^n
bootstrap_probs <- data_frame(n=numeric(),probability = numeric())
bootstrap_probs$n <- 1:100000
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1- 1/n)^n
bootstrap_probs <- data_frame(n=1:100000,probability = numeric())
#From textbook questions, the probability that any arbitrary O_j observation will be in a bootstrap resample of size n is
# 1 - (1- 1/n)^n
bootstrap_probs <- data_frame(n=1:100000,probability = numeric(100000))
bootstrap_probs$probability <- 1 - (1- 1/(bootstrap_probs$n) )^(bootstrap_probs$n)
bootstrap_probs
ggplot(bootstrap_probs,aes(x=n , y=probability)) + geom_point()
ggplot(bootstrap_probs,aes(x=n , y=probability)) + geom_smooth()
ggplot(bootstrap_probs,aes(x=n , y=probability)) + geom_point()
ggplot(bootstrap_probs,aes(x=n , y=probability)) +y_lim(0,1)+ geom_point()
ggplot(bootstrap_probs,aes(x=n , y=probability)) +ylim(0,1)+ geom_point()
library(tidyverse)
library(ISLR)
library(boot)
library(tidyverse)
glm_fit <- glm(data = Default , default ~ income + balance)
glm.fit()
Default
glimpse(Default)
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial')
glm_fit
set.seed(1)
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial')
?sample
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) )
test <- !train
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial' , subset = train)
test <- Default[!train,]
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
?predict
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test )
glm_predictions
test
test <- Default[!train,]
test
Default
train
test <- Default[train, ]
test
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) ) %>% arrange()
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) ) %>% order()
train
test <- Default[-train, ]
#train model on training set
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial' , subset = train)
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test )
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
glm_predictions
Default
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response") %>%
case_when(.x>0.5 ~ "Yes",
TRUE ~ "No")
glm_predictions <- predict(glm_fit , newdata = test , type = "response") %>%
case_when(.>0.5 ~ "Yes",
TRUE ~ "No")
glm_predictions <- predict(glm_fit , newdata = test , type = "response") %>%
case_when(.>= 0.5 ~ "Yes",
TRUE ~ "No")
glm_predictions <-  case_when(glm_predictions>= 0.5 ~ "Yes",
TRUE ~ "No")
test_error_rate <- mean(glm_predictions!= test$default)
validation_mse <- function(){
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) )
test <- Default[-train, ]
#train model on training set
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial' , subset = train)
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
glm_predictions <-  case_when(glm_predictions>= 0.5 ~ "Yes",
TRUE ~ "No")
test_error_rate <- mean(glm_predictions!= test$default)
return(test_error_rate)
}
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse()
validation_mse_student <- function(){
#define train and test set
train <- sample(dim(Default)[1] , dim(Default)[1]/2  %>% round(.,0) )
test <- Default[-train, ]
#train model on training set
glm_fit <- glm(data = Default , default ~ income + balance + student, family = 'binomial' , subset = train)
#predict on test set
glm_predictions <- predict(glm_fit , newdata = test , type = "response")
glm_predictions <-  case_when(glm_predictions>= 0.5 ~ "Yes",
TRUE ~ "No")
test_error_rate <- mean(glm_predictions!= test$default)
return(test_error_rate)
}
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
validation_mse_student()
#Will use resampling to calculate variability of the beta coefficients of the glm model in the previous question
glm_fit <- glm(data = Default , default ~ income + balance , family = 'binomial')
#look at values for coefficients
summary(glm_fit)
boot_fn(data , index){
glm_fit <- glm(data = data , default ~ income + balance , family = 'binomial' , subset = index)
return(glm_fit$coefficients)
}
boot(Default, boot_fn , R=1000)
boot_fn <- function(data , index){
glm_fit <- glm(data = data , default ~ income + balance , family = 'binomial' , subset = index)
return(glm_fit$coefficients)
}
boo
boot_fn <- function(data , index){
glm_fit <- glm(data = data , default ~ income + balance , family = 'binomial' , subset = index)
return(glm_fit$coefficients)
}
boot(Default, boot_fn , R=1000)
?boot
boot(Default, boot_fn , R=1000 ,parallel = "snow")
boot_results <- boot(Default, boot_fn , R=1000 ,parallel = "snow")
glm_fit$coefficients
summary(glm_fit)
#Doing LOOCV MANUALLY NOW
glm_fit <- glm(data = Weekly , Direction ~ Lag1 + Lag2)
#Doing LOOCV MANUALLY NOW
glm_fit <- glm(data = Weekly , Direction ~ Lag1 + Lag2 , family = "binomial")
dim(Weekly)[1]-1
glm_fit_less1 <- glm(data = Weekly , Direction ~ Lag1 + Lag2 , family = "binomial" , subset = c(FALSE,rep(TRUE,dim(Weekly)[1]-1)))
#Use the second model to predict the direction or Y variable for the FIRST observation (This observation was not included in training set)
predict(glm_fit_less1 , newdata = Weekly[1,] , type = "response")
glm_fit_less1 <- glm(data = Weekly[-1,] , Direction ~ Lag1 + Lag2 , family = "binomial")
#Use the second model to predict the direction or Y variable for the FIRST observation (This observation was not included in training set)
predict(glm_fit_less1 , newdata = Weekly[1,] , type = "response") > 0.5
Weekly
n <- dim(Weekly)[1]
actual <- case_when(Weekly[5,"Direction"]=="Up" ~ TRUE,
TRUE ~ FALSE)
actual
n <- dim(Weekly)[1]
#define vector to record if there was a prediction error
error <- numeric(n)
for ( i in 1:n){
glm_fit_less1 <- glm(data = Weekly[-i,] , Direction ~ Lag1 + Lag2 , family = "binomial")
prediction <- predict(glm_fit_less1 , newdata = Weekly[i,] , type = "response") > 0.5
actual <- case_when(Weekly[i,"Direction"]=="Up" ~ TRUE,
TRUE ~ FALSE)
if(actual==prediction){
error[i] <- 0
}else{
error[i] <- 1
}
}
mse_loocv <- mean(error)
mse_loocv
sum(error)
#Simulate data
set.seed(1)
x <- rnorm(100)
y = x -2*x^2 + rnorm(100)
y <-  x -2*x^2 + rnorm(100)
ggplot(aes(x=x,y=y))
ggplot(data=c(x,y),aes(x=x,y=y))
plot(y~x)
dat <- data_frame(x=x,y=y)
#Produce LOOCV  errors for polynomial models up to order 4
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
errors <- numeric(n)
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
for (degree in 1:4){
errors <- numeric(n)
for ( i in 1:n){
lm_fit_less1 <- lm(data = dat[-i,] , y ~ poly(x,degree) )
prediction <- predict(lm_fit_less1 , newdata = dat[i,] , type = "response")
errors[i] <- (prediction - dat[i,"y"])^2
}
mse_errors$degree <- mean(errors)
}
mse_errors
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
for (degree in 1:4){
errors <- numeric(n)
for ( i in 1:n){
lm_fit_less1 <- lm(data = dat[-i,] , y ~ poly(x,degree) )
prediction <- predict(lm_fit_less1 , newdata = dat[i,] , type = "response")
errors[i] <- (prediction - dat[i,"y"])^2
}
mse_errors[degree] <- mean(errors)
}
mse_errors
set.seed(200)
n <- dim(dat)[1]
#define vector to record if there was a prediction error
mse_errors <- list()
for (degree in 1:4){
errors <- numeric(n)
for ( i in 1:n){
lm_fit_less1 <- lm(data = dat[-i,] , y ~ poly(x,degree) )
prediction <- predict(lm_fit_less1 , newdata = dat[i,] , type = "response")
errors[i] <- (prediction - dat[i,"y"])^2
}
mse_errors[degree] <- mean(errors)
}
mse_errors
mse_errors <- list()
for (degree in 1:4){
lm_fit <- glm(data = dat , y ~ poly(x,degree) )
mse_errors[degree] <- cv.glm(lm_fit_less1,data = dat )$delta[1]
}
mse_errors
lm_fit <- glm(data = dat , y ~ poly(x,1) )
lm_fit
mse_errors <- list()
for (degree in 1:4){
lm_fit <- glm(data = dat , y ~ poly(x,degree) )
mse_errors[degree] <- cv.glm(lm_fit,data = dat )$delta[1]
}
mse_errors
library(MASS)
Boston$medv
mu_estimate <- mean(Boston$medv)
X_bar_deviation <- sd(Boston$medv) / sqrt(length(Boston$medv))
?boot
boot_fn <- function(data, index){
return(mean(data[index,"medv"]))
}
boot(Boston,boot_fn , R=1000)
?predict
#Prediction of 24.47mpg
#Confidence Interval vs Prediction Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval =c("confidence","prediction"))
library(tidyverse)
library(ISLR)
#Question 8-----------
lm_model <-  lm(data = Auto , mpg ~ horsepower)
summary(lm_model)
#Prediction of 24.47mpg
#Confidence Interval vs Prediction Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval =c("confidence","prediction"))
#Prediction of 24.47mpg
#Confidence Interval vs Prediction Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="prediction")
#Confidence Interval
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="confidence")
summary(lm_model)
predict(lm_model , newdata = data_frame(horsepower = 98)) +2*4.906
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="prediction")
dim(Auto)
predict(lm_model , newdata = data_frame(horsepower = 98)) +2*(4.906/sqrt(392))
#Confidence Interval this is for Y_bar
predict(lm_model , newdata = data_frame(horsepower = 98) , interval ="confidence")
#plot results
plot(data = Auto , x = horsepower , y =mpg)
?plot
#plot results
plot(Auto , x = horsepower , y =mpg)
#plot results
plot( x = Auto$horsepower , y =Auto$mpg)
abline(lm_model)
abline(lm_model , col = "red")
#Look at diagnostic plots
plot(lm_model)
install.packages("gvlma")
library(gvlma)
gvlma::gvlma(lm_model)
diagnostics <- gvlma::gvlma(lm_model)
summary(diagnostics)
plot(diagnostics)
?gvlma
#Question 9---------
pairs(Auto)
#Question 9---------
pairs(Auto[,-"name"])
colnames(Auto)
#Question 9---------
pairs(Auto[,-9])
cor(Auto[,-9])
lm_fit_2 <- lm(data = Auto , mpg ~ . -name)
summary(lm_fit_2)
#Remember, a point has to have a mix of high leverage and outlier-ness to be INFLUENTIAL
#Let's look at diagnostic plots
plot(lm_fit_2)
plt(gvlma(lm_fit_2))
plot(gvlma(lm_fit_2))
diagnostics_2 <- (gvlma(lm_fit_2))
plot(diagnostics_2)
#Remember, a point has to have a mix of high leverage and outlier-ness to be INFLUENTIAL
#Let's look at diagnostic plots
plot(lm_fit_2)
plot(lm_fit_2)
diagnostics_2
lm_fit_2_interactions <- lm(data = Auto , mpg ~ (. -name)^8)
lm_fit_2_interactions
lm_fit_2_interactions <- lm(mpg~cylinders*displacement+displacement*weight , data = Auto)
#By looking at correlation matrix, try these combinations
#Also have a look at the notes on formula notation in R
summary(lm_fit_2_interactions)
# We can clearly see non-normal relationship betwen mpg and regressors. Let's try a log transformation
lm_fit_2_log <- lm(log(mpg)~cylinders*displacement+displacement*weight , data = Auto)
library(tidyverse)
library(ISLR)
library(gvlma) #
# We can clearly see non-normal relationship betwen mpg and regressors. Let's try a log transformation
lm_fit_2_log <- lm(log(mpg)~cylinders*displacement+displacement*weight , data = Auto)
summary(lm_fit_2_log)
lm_fit_2_interactions <- lm(mpg~cylinders*displacement+displacement*weight , data = Auto)
#By looking at correlation matrix, try these combinations
#Also have a look at the notes on formula notation in R
summary(lm_fit_2_interactions)
#Is this better than the interactions only model?
anova(lm_fit_2_interactions,lm_fit_2_log)
carseats_model <- lm(data = Carseats , Sales ~ Price + Urban  + US)
summary(carseats_model)
summary(Carseats)
#from coefficients, only price and US seem siginificant
update.formula(carseats_model, Sales ~ Price + US)
carseats_model
#from coefficients, only price and US seem siginificant
carseats_improved <-  lm(data = Carseats , Sales ~ Price   + US)
#Get 95% confidence intervals for coefficients
confint(carseats_improved)
library(boot)
#Compare to bootstrap
boot_fn <- function(data , index){
return({
coef(lm(data = data , Sales ~ Price   + US))
})
}
boot(Carseats, boot_fn , R =1000)
boot_fn <- function(data , index){
return({
coef(lm(data = data , Sales ~ Price   + US , subset= index))
})
}
boot(Carseats, boot_fn , R =1000)
#Get 95% confidence intervals for coefficients
confint(carseats_improved)
#Close results
plot(gvlma(carseats_improved))
#Close results
par("mar")
#Close results
par(mar)
#Close results
par('mar')
par(mar=c(1,1,1,1))
plot(gvlma(carseats_improved))
par(mar=c(1.5,1.5,1.5,1.5))
plot(gvlma(carseats_improved))
?par
#Investigate the t-statistic for individual beta coefficients
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)
dat <- data_frame(x=x,y=y)
interceptless_model <- lm (data = dat , y~x -1)
interceptless_model
summary(interceptless_model)
y_onto_x <- lm (data = dat , y~x -1) #without intercept
summary(y_onto_x)
x_onto_y<- lm (data = dat , x~y -1) #without intercept
summary(x_onto_y)
summary(y_onto_x)
